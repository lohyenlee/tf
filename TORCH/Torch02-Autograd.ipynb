{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87609550-5469-42e9-9bda-24378616d02f",
   "metadata": {},
   "source": [
    "# Neural Networks in PyTorch\n",
    "## Chapter 2: Automatic Differentiation\n",
    "Yen Lee Loh, 2021-9-1; 2022-9-22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e3fa2-f789-407c-af2c-f739505c0264",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 0.  Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aef155d-580e-4294-a1ed-4fdc91c370bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update ({'font.family':'serif', 'font.size':14})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d951642c-3969-4222-887d-b48afa86af45",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Automatic differentiation\n",
    "Let $x=2$ and $Y=\\tanh x$.  Now suppose we want to calculate $dY/dx$.\n",
    "\n",
    "In a **symbolic differentiation** approach, one would get the computer to calculate the derivative analytically as $dY/dx={\\rm sech}^2 x$, and then substitute in the value of $x$ to obtain $dY/dx_{x=2}=0.07$.\n",
    "\n",
    "In an **automatic differentiation** approach, used in most implementations of artificial neural networks, the computer does not need to do any symbolic calculus.  Instead, every function \"knows\" its own derivative.  For example, when PyTorch calculates $Y=\\tanh x$ for a certain value of $x$, PyTorch also calculates $dY/dx = {\\rm sech}^2 x$ for that value of $x$.  It is likely that the actual implementation calculates $dY/dx = 1-Y^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e3f815f-0f40-4eb7-8f7c-aa9e80745bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)   # input  x = 2\n",
    "Y = torch.tanh(x)                           # output Y = tanh(x)\n",
    "Y.backward()      # calculate gradient of Y with respect to everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc9f97c3-87f8-4934-a832-fd1c09dbbd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x         = tensor(2., requires_grad=True)\n",
      "Y         = tensor(0.9640, grad_fn=<TanhBackward0>)\n",
      "dY/dx     = tensor(0.0707)\n"
     ]
    }
   ],
   "source": [
    "print (\"x         =\", x)\n",
    "print (\"Y         =\", Y)\n",
    "print (\"dY/dx     =\", x.grad)  # the gradient of Y with respect to x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1717773c-47f3-41e5-aef9-3daa02f46345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sech^2(x) = tensor(0.0707, grad_fn=<MulBackward0>)\n",
      "1-Y**2    = tensor(0.0707, grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print (\"sech^2(x) =\", 1/torch.cosh(x)**2)\n",
    "print (\"1-Y**2    =\",1-Y**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a675eacc-51a9-4ad8-aecf-1021a1aced9b",
   "metadata": {},
   "source": [
    "The example below does the same thing, except that $f$ is a neural network layer, and we feed in the input $x$ to get the output $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "604a237d-7277-49a1-8f4f-31df4a471984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x         = tensor(2., requires_grad=True)\n",
      "Y         = tensor(0.9640, grad_fn=<TanhBackward0>)\n",
      "dY/dx     = tensor(0.0707)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True) # input           x = 2\n",
    "f = nn.Tanh()                             # define a layer  f(x) = tanh(x)\n",
    "Y = f(x)                                  # output          Y = tanh(2)\n",
    "#f.zero_grad()\n",
    "Y.backward()\n",
    "print (\"x         =\", x)\n",
    "print (\"Y         =\", Y)\n",
    "print (\"dY/dx     =\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e573604-c049-4770-a10a-119c82a9237b",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Chain rule\n",
    "Let $x=2$, $y=\\tanh x$, and $z=\\tanh y$.  Then\n",
    "$\\frac{dz}{dx} = \\frac{dy}{dx} \\frac{dz}{dy}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbadc949-ff50-4cb1-8869-902a85580b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x              = tensor(2., requires_grad=True)\n",
      "y              = tensor(0.9640, grad_fn=<TanhBackward0>)\n",
      "z              = tensor(0.7461, grad_fn=<TanhBackward0>)\n",
      "dz/dy (auto)   = tensor(0.4434)\n",
      "dz/dx (auto)   = tensor(0.0313)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor (2.0, requires_grad=True) # set       x = 2\n",
    "y = torch.tanh (x) ; y.retain_grad()       # calculate y = tanh(x)\n",
    "z = torch.tanh (y)                         # calculate z = tanh(y)\n",
    "z.backward()                               # calculate dz/dy = sech^2(y)\n",
    "                                           # and       dy/dx = sech^2(x)\n",
    "                                           # and thus  dz/dx\n",
    "print (\"x              =\", x)\n",
    "print (\"y              =\", y)\n",
    "print (\"z              =\", z)\n",
    "print (\"dz/dy (auto)   =\", y.grad)\n",
    "print (\"dz/dx (auto)   =\", x.grad)\n",
    "print ()\n",
    "# print (\"du/dx (manual) = 1-u**2 =\", 1-u**2)\n",
    "# print (\"dv/du (manual) = 1-v**2 =\", 1-v**2)\n",
    "# print (\"dv/dx (manual) = (1-u**2)*(1-v**2) =\", (1-u**2)*(1-v**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e6b1e-bcc2-4670-b04f-22325eb68997",
   "metadata": {},
   "source": [
    "The code below does essentially the same thing, for a network of the form\n",
    "\n",
    "    x -------(tanh)-------> y --------(tanh)--------> z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8adfdf12-12d0-412a-bb90-7f8f71d7b656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x         = tensor(2., requires_grad=True)\n",
      "z         = tensor(0.7461, grad_fn=<TanhBackward0>)\n",
      "dz/dx     = tensor(0.0313)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "f = nn.Sequential (nn.Tanh(), nn.Tanh())\n",
    "z = f(x)\n",
    "z.backward()\n",
    "print (\"x         =\", x)\n",
    "print (\"z         =\", z)\n",
    "print (\"dz/dx     =\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a87dc-41e4-4a64-a858-75990f6f8a7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 3. Layers with learnable parameters\n",
    "Let $x=2$ and $y=wx+b$, where $x$ is the input, $y$ is the output, and $w$ and $b$ are parameters.  For our present purposes, we may think of $w$ and $b$ as additional inputs:\n",
    "\n",
    "<img src=\"SKETCHES/linearlayer2.png\"/>\n",
    "\n",
    "Then, automatic differentiation produces\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x} = w$$\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial w} = x$$\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial b} = 1$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "085e1241-6887-46fe-9cd9-f727179d981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x         =  tensor(2., requires_grad=True)\n",
      "w         =  tensor(3., requires_grad=True)\n",
      "b         =  tensor(4., requires_grad=True)\n",
      "y         =  tensor(10., grad_fn=<AddBackward0>)\n",
      "dy/dx = w =  tensor(3.)\n",
      "dy/dw = x =  tensor(2.)\n",
      "dy/db = 1 =  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "w = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(4.0, requires_grad=True)\n",
    "y = w * x + b\n",
    "y.backward()\n",
    "print (\"x         = \", x)\n",
    "print (\"w         = \", w)\n",
    "print (\"b         = \", b)\n",
    "print (\"y         = \", y)\n",
    "print (\"dy/dx = w = \", x.grad)\n",
    "print (\"dy/dw = x = \", w.grad)\n",
    "print (\"dy/db = 1 = \", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d178f-c632-4f98-b8c8-c2842021df38",
   "metadata": {
    "tags": []
   },
   "source": [
    "Later, we will see that training a network involves adjusting parameters (such as $w$ and $b$) according to the derivatives of the objective function with respect to those parameters (such as $\\frac{\\partial \\varepsilon}{\\partial w}$ and $\\frac{\\partial \\varepsilon}{\\partial b}$).  These derivatives can collectively be referred to as the **gradient** of the objective function.  Computing the gradient of the objective function using automatic differentiation with the chain rule is referred to as **backpropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e93ab-670d-43bf-80af-e8b07c86172f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 4. Layers with multiple parameters\n",
    "A general linear layer is of the form ${\\bf Y} = {\\bf W}\\cdot {\\bf X} + {\\bf B}$.  That is,\n",
    "\n",
    "$$Y_{i} = \\sum_j W_{ij} X_{j} + B_i.$$\n",
    "\n",
    "The partial derivatives are\n",
    "\n",
    "$$\\frac{\\partial Y_i}{\\partial X_j} = W_{ij} $$\n",
    "\n",
    "$$\\frac{\\partial Y_k}{\\partial W_{ij}} = \\delta_{ki} X_{j} $$\n",
    "\n",
    "$$\\frac{\\partial Y_i}{\\partial B_i} = 1. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d3c9b1-d286-44a8-a637-103e417ffeff",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 5. Example exercise\n",
    "Consider a neural network implementing the function ${\\bf Y} = \\tanh( {\\bf W}\\cdot {\\bf X} + {\\bf B} )$.  Find the partial derivatives of the vector ${\\bf Y}$ with respect to the parameters ${\\bf W}$ and ${\\bf B}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
