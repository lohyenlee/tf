{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9927ae0-bfe9-40fe-a3dc-6042b1fc4d13",
   "metadata": {},
   "source": [
    "# Neural Networks using NumPy\n",
    "*Jacob Nelson and Yen Lee Loh, 2021-5-31; 2022-12-30*\n",
    "\n",
    "This Jupyter Lab notebook demonstrates how one may construct simple neural networks using pure NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7217f49-59e0-45a9-9a28-10f364922298",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Single-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6380142e-96bf-41cc-9886-11c94c8c0b91",
   "metadata": {},
   "source": [
    "Consider a single-layer perceptron described by the feedforward equations\n",
    "$u_1 = x_0 w_{01}, x_1 = \\tanh u_1, F=(x_1-y)^2$:\n",
    "\n",
    "    x0  ----------------> u1 ----------------> x1 ----------------> F\n",
    "    w01 ---------------/                       y  --------------/\n",
    "\n",
    "Here x0 is the input, w01 are the weights, u1 is a hidden node, x1 is the model output, y is the training output, and F is the loss.  We may calculate the gradient of F with respect to every node using backpropagation:\n",
    "\n",
    "    dF/dx0  <-------- dF/du1 <------------- dF/dx1 <-------------- dF/dF=1\n",
    "    dF/dw01 <--------/                      dF/dy  <--------------/\n",
    "\n",
    "This allows us to calculate the gradient of $F$ with respect to the learnable parameters (weights), which allow us to train the network by adjusting the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5655ce1c-3a27-4fe5-b410-a438979d6a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Initial weights W   Training outputs Y                      \n",
      "          [-1.4 +1.3 -0.9]    [-1  1  1  1]                           \n",
      "\n",
      "Epoch     Weights W           Model outputs x1              Loss F    \n",
      "0         [-1.4  1.3 -0.8]    [-0.7 +0.4 -1.0 -0.8]         7.5486\n",
      "1000      [ 2.4  2.5 -1.1]    [-0.8 +0.9 +0.9 +1.0]         0.0695\n",
      "2000      [ 2.9  2.9 -1.3]    [-0.9 +0.9 +0.9 +1.0]         0.0335\n",
      "3000      [ 3.1  3.1 -1.4]    [-0.9 +0.9 +0.9 +1.0]         0.0218\n",
      "4000      [ 3.2  3.2 -1.5]    [-0.9 +0.9 +0.9 +1.0]         0.0161\n",
      "5000      [ 3.4  3.4 -1.6]    [-0.9 +0.9 +0.9 +1.0]         0.0127\n",
      "6000      [ 3.5  3.5 -1.6]    [-0.9 +1.0 +1.0 +1.0]         0.0105\n",
      "7000      [ 3.5  3.5 -1.7]    [-0.9 +1.0 +1.0 +1.0]         0.0089\n",
      "8000      [ 3.6  3.6 -1.7]    [-0.9 +1.0 +1.0 +1.0]         0.0078\n",
      "9000      [ 3.7  3.7 -1.7]    [-0.9 +1.0 +1.0 +1.0]         0.0069\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions (precision=4)\n",
    "rng = np.random.default_rng(12345)\n",
    "def mystr(a): return np.array2string(a.flatten().round(1), formatter={'float_kind': '{:+.1f}'.format})\n",
    "def sech2(x): return np.cosh(x)**(-2)\n",
    "\n",
    "xnd = np.array( [[0,0,1],[0,1,1],[1,0,1],[1,1,1]] ) # training inputs (4x3)\n",
    "ynd = np.array( [[-1],   [1],    [1],    [1]    ] ) # training outputs (4x1)\n",
    "N,D0 = xnd.shape\n",
    "N,D1 = ynd.shape\n",
    "\n",
    "w01 = rng.normal(size=(D0,D1))  # random weights\n",
    "tau = 0.01                      # learning rate\n",
    "print ('{:10}{:20}{:30}{:10}'.format ('', 'Initial weights W', 'Training outputs Y', ''))\n",
    "print ('{:10}{:20}{:30}{:10}'.format ('', mystr(w01), mystr(ynd), ''))\n",
    "print ()\n",
    "print ('{:10}{:20}{:30}{:10}'.format ('Epoch', 'Weights W', 'Model outputs x1', 'Loss F'))\n",
    "#======== Loop over training epochs\n",
    "for t in range(10000):\n",
    "  #======== Load a batch of training data (in this case we just work with the entire dataset)\n",
    "  x0 = xnd                    # N*D0\n",
    "  y = ynd                     # N*D1\n",
    "  #======== Feedforward\n",
    "  u1 = x0 @ w01               # N*D1\n",
    "  x1 = np.tanh (u1)           # N*D1\n",
    "  F = (x1-y).T @ (x1-y)       # scalar\n",
    "  #======== Backpropagate\n",
    "  dFdx1  = 2*(x1-y)           # N*D1\n",
    "  dFdu1  = sech2(u1) * dFdx1  # N*D1\n",
    "  dFdx0  = w01 @ dFdu1.T      # N*D0 (actually we didn't need to compute this)\n",
    "  dFdw01 = x0.T @ dFdu1       # D0*D1, obtained from (D0*N) dotted with (N*D1)\n",
    "  #======== Train\n",
    "  w01 -= tau * dFdw01\n",
    "  if t%1000==0:\n",
    "    print ('{:<10d}{:20s}{:30s}{:.4f}'.format( t, str(w01.flatten().round(1)), mystr(x1), F[0,0] ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b9dc6-301f-4d69-9c86-4cfdd1372709",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Example 1: Single-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8938aa7-9428-4107-88ed-68ee9d9ffaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training neural network ...\n",
      "\n",
      "Iteration        0\tError e1 = 0.5853\n",
      "Iteration    10000\tError e1 = 0.0092\n",
      "Iteration    20000\tError e1 = 0.0065\n",
      "\n",
      "Weights:\n",
      "\n",
      "\tw1\tw2\tw3\n",
      "\t-------\t-------\t-------\t-------\t-------\t\n",
      "\t9.805\t9.805\t-4.670\t\n",
      "\n",
      "Training dataset and prediction accuracy:\n",
      "\n",
      "\tx1\tx2\tx3\tYPred\tyTrain\n",
      "\t-------\t-------\t-------\t-------\t-------\t\n",
      "\t0.000\t0.000\t1.000\t0.009\t0.000\t\n",
      "\t0.000\t1.000\t1.000\t0.994\t1.000\t\n",
      "\t1.000\t0.000\t1.000\t0.994\t1.000\t\n",
      "\t1.000\t1.000\t1.000\t1.000\t1.000\t\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def printTable(a):           # a is a numpy.array\n",
    "    [imax,jmax] = np.shape(a);\n",
    "    for i in range(imax):\n",
    "        print (\"\\t\", end='')\n",
    "        for j in range(jmax): \n",
    "            print (\"{:.3f}\".format(a[i,j]), end='\\t')\n",
    "        print ()\n",
    "\n",
    "def g(x): return 1/(1+np.exp(-x)) # g is a sigmoid function\n",
    "def h(g): return g*(1-g)          # h is such that g'(x) = h(g(x))\n",
    "\n",
    "X = np.array( [[0,0,1],[0,1,1],[1,0,1],[1,1,1]] )  # define training inputs\n",
    "Y = np.array( [[0,1,1,1]] ).T                      # define training outputs (column vector)\n",
    "nmax,dmax = X.shape\n",
    "\n",
    "print (\"\\nTraining neural network ...\\n\")\n",
    "np.random.seed(1)\n",
    "w0 = 2*np.random.random((3,1)) - 1   # init weights randomly\n",
    "for j in range(30000):\n",
    "    x0 = X               # entire set of training data\n",
    "    x1 = g( x0.dot(w0) ) # feedforward to level 1 neurons (entire dataset at once)\n",
    "    e1 = Y - x1          # level 1 error\n",
    "    d1 = e1 * h(x1)      # level 1 delta\n",
    "    w0 += x0.T.dot(d1)   # update layer0-layer1 couplings\n",
    "    if (j%10000) == 0:\n",
    "        print (\"Iteration {:8d}\\tError e1 = {:6.4f}\".format (j, np.mean(np.abs(e1))  )) \n",
    "\n",
    "print (\"\\nWeights:\\n\")\n",
    "print (\"\\tw1\\tw2\\tw3\")\n",
    "print (\"\\t-------\\t-------\\t-------\\t-------\\t-------\\t\")\n",
    "printTable (w0.T)\n",
    "        \n",
    "print (\"\\nTraining dataset and prediction accuracy:\\n\")\n",
    "print (\"\\tx1\\tx2\\tx3\\tYPred\\tyTrain\")\n",
    "print (\"\\t-------\\t-------\\t-------\\t-------\\t-------\\t\")\n",
    "printTable (np.hstack ((X, x1, Y)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854292f-ccc8-468e-af8e-ad44876bbc46",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0be13e-c15c-43cf-89c1-bdfb6bde1824",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Example 2: Two-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac0cf546-4c44-4813-9332-a8e9fc7c4797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration        0\tError e1 = 0.0813\n",
      "Iteration    10000\tError e1 = 0.0005\n",
      "Iteration    20000\tError e1 = 0.0003\n",
      "Iteration    30000\tError e1 = 0.0002\n",
      "Iteration    40000\tError e1 = 0.0001\n",
      "Iteration    50000\tError e1 = 0.0001\n",
      "\n",
      "Weights w0 (between input and hidden layer):\n",
      "\n",
      "\t4.601\t4.172\t-6.310\t-4.197\t\n",
      "\t-2.584\t-5.814\t-6.608\t-3.684\t\n",
      "\t0.975\t-2.027\t2.529\t5.844\t\n",
      "\n",
      "Weights w1 (between hidden layer and output):\n",
      "\n",
      "\t-6.968\t\n",
      "\t7.141\t\n",
      "\t-10.319\t\n",
      "\t7.861\t\n",
      "\n",
      "Training dataset and prediction accuracy:\n",
      "\n",
      "\tx1\tx2\tx3\tz1\tz2\tz3\tz4\tYPred\tyTrain\n",
      "\t-------\t-------\t-------\t-------\t-------\t-------\t-------\t-------\t--------\n",
      "\t0.000\t0.000\t1.000\t0.726\t0.116\t0.926\t0.997\t0.003\t0.000\t\n",
      "\t0.000\t1.000\t1.000\t0.167\t0.000\t0.017\t0.897\t0.997\t1.000\t\n",
      "\t1.000\t0.000\t1.000\t0.996\t0.895\t0.022\t0.838\t0.997\t1.000\t\n",
      "\t1.000\t1.000\t1.000\t0.952\t0.025\t0.000\t0.115\t0.004\t0.000\t\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1]])\n",
    "Y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "np.random.seed(1)\n",
    "w0 = 2*np.random.random((3,4)) - 1   # weights from layer 0 to layer 1 (matrix)\n",
    "w1 = 2*np.random.random((4,1)) - 1   # weights from layer 1 to layer 2 (matrix)\n",
    "\n",
    "for j in range(60000):\n",
    "    x0 = X\n",
    "    x1 = g( x0.dot(w0) )  # Feedforward through layer 1\n",
    "    x2 = g( x1.dot(w1) )  # Feedforward to layer 2\n",
    "    e2 = Y - x2           # Error at level 2\n",
    "    d2 = e2 * h(x2)       # Check confidence, mute confidence answers\n",
    "    \n",
    "    e1 = d2.dot (w1.T)    # Backpropagate: how much did each x1 value contribute to e2?    \n",
    "    d1 = e1 * h(x1)       # Check confidence of x1\n",
    "    \n",
    "    w1 += x1.T.dot(d2)    # Update weights\n",
    "    w0 += x0.T.dot(d1)\n",
    "   \n",
    "    if (j%10000) == 0:\n",
    "        print (\"Iteration {:8d}\\tError e1 = {:6.4f}\".format (j, np.mean(np.abs(e1))  )) \n",
    "\n",
    "print (\"\\nWeights w0 (between input and hidden layer):\\n\")\n",
    "printTable (w0)\n",
    "print (\"\\nWeights w1 (between hidden layer and output):\\n\")\n",
    "printTable (w1)\n",
    "print (\"\\nTraining dataset and prediction accuracy:\\n\")\n",
    "print (\"\\tx1\\tx2\\tx3\\tz1\\tz2\\tz3\\tz4\\tYPred\\tyTrain\")\n",
    "print (\"\\t-------\\t-------\\t-------\\t-------\\t-------\\t-------\\t-------\\t-------\\t--------\")\n",
    "printTable (np.hstack ((X, x1, x2, Y)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d9700-f7b4-4812-9818-a04bd76335ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
